[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zhuoyuan Tang–eportfolio",
    "section": "",
    "text": "Welcome\nThis Is Zhuoyuan’s EPortfolio\nEnjoy It and Have Fun!!!\n(Continue to Update ……)",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "1  CV",
    "section": "",
    "text": "Mr. Zhuoyuan Tang\nAddress: Longyuan Community, Carlsberg Avenue, Dali City, Yunnan Province, P.R. China\nMobile: 8613987298711\nEmail: k24026267@kcl.ac.uk\nMy Google Scholar Profile:  Zhuoyuan Tang \n\n\n\n\n\n\n\n\n\n\nSummary Statement\n\n\n\nAn enthusiast in the fields of data science and machine learning, with a profound academic background and practical (work) experience in algorithm development, computer vision, and statistical modeling etc. A learner/researcher who has published research results, specializing in optimization and predictive modeling, and also a person who is good at teamwork with enthusiastic and sincere!\n\n\n\nEDUCATIONAL BACKGROUND\n\n\n\n\n\n\n\n\nEDUCATIONAL BACKGROUND\n\n\n\n\n\n\n\n\nKing’s College London (KCL) —— London, UK\nMSc Applied Statistical Modelling and Health Informatics (Sept. 2024 – Now)\n▪ Predicted Degree Classification: Merit or Distinction\n\n\n \n\n\nXi’an Jiaotong-Liverpool University (XJTLU) —— Suzhou,China\nBachelor of Data Science and Big Data Technology (Sept. 2020 – Jun. 2024)\n▪ GPA: 71.025/100 (The final dissertation score is 84/100)\n▪ Degree Classification: First Class (Honours)\n▪ Scholarship: Best Performance in Final Year Project\n\n\n \n\n\n\n\n\n\nPUBLICATION\n\n\n\n\n\n\n\n\nPUBLICATION\n\n\n\n\n\n\nTang, Z., Hasan, M. M., Strauss, T. (2025). Optimized YOLOv5 Model for Safety Helmet and Flame Detection System. https://doi.org/10.1007/s11760-025-04073-z\nTang, Z. (2023). Prediction of the Status of the Hotel Reservations. Applied and Computational Engineering, 8(1), pp.734–743. https://doi.org/10.54254/2755-2721/8/20230154\n\n\n\n\n\nRESEARCH EXPERIENCE\n\n\n\n\n\n\n\n\nRESEARCH EXPERIENCE\n\n\n\n\n\n\nKing’s College London Research Assistant   (08/2025-Current)\n\nA cutting-edge research project focused on developing fully automated approaches to depression severity detection by combining textual and facial expression data. Leveraging the E-DAIC dataset, the study employs Large Language Models (LLMs) to extract clinical indicators of depression from interview transcripts, with corresponding PHQ-8 scores serving as ground truth. The project also incorporates facial feature extraction from video data to build a multimodal model for depression prediction and will produce an academic paper.\nKey findings from preliminary experiments show that enhancing text data with speech quality measures leads to superior predictive performance, highlighting the promise of multimodal approaches in digital mental health diagnostics.\nKey Responsibilities:\n▪ Develop and fine-tune LLM-based models to extract depression-related features from clinical interview transcripts.\n▪ Extract and process facial expression features from video data using state-of-the-art computer vision techniques.\n▪ Design and implement multimodal fusion architectures combining text and facial data for depression severity prediction.\n▪ Evaluate model performance using metrics such as Mean Absolute Error (MAE) and Root Mean Square Error (RMSE).\n▪ Conduct error analysis and ablation studies to identify model strengths and limitations.\n▪ Document code, contribute to publications, and assist with dissemination of results.\n▪ Collaborate with interdisciplinary researchers in AI, psychology, and clinical informatics.\n\nApplication and Practice of Data Science   (01/2023-02/2023)\n\n▪ Studied data science fundamentals, applications, and visualisations, collaborating within a multicultural environment with an international team and proficiently applied big data processing and analysis techniques.\n▪ Utilised machine learning models for data classification and prediction, optimised model performance, enhanced problem-solving abilities through innovative approaches and applied modern data analysis methods to effectively address complex research challenges, successfully solving practical problems in novel datasets.\n▪ Published research findings in a paper in CPCI (an international indexing platform).\n\n\n\n\nINTERNSHIP EXPERIENCE\n\n\n\n\n\n\n\n\nINTERNSHIP EXPERIENCE\n\n\n\n\n\n\n1. China Telecom Stocks Co., Ltd.   (06/2023-09/2023)   Algorithm Development\n\n▪ Created a virtual environment in a Conda environment on a remote server via SSH, ensuring a stable and isolated workspace for algorithm development and testing.\n▪ Mastered the deployment and operation of the Milvus vector database, successfully integrated it into the system architecture for efficient storage and retrieval of vector data.\n▪ Developed and implemented computer vision models based on the YOLO open-source framework and performed object detection tasks such as helmet detection, fall detection, and smoking detection using images, videos, and real-time surveillance data.\n▪ Strengthened proficiency in Python programming, gained hands-on experience in algorithm development and machine learning applications, which deepened understanding of computer vision techniques and their practical uses in business contexts.\n\n2. Tencent Cloud Computing Co., Ltd.   (06/2022-07/2022)   App Developer\n\n▪ Familiarised with the regular app development process, gaining insights into the complete lifecycle of mobile application development, from requirements gathering to deployment.\n▪ Mastered Android Studio development software and Java programming language, acquiring the technical skills necessary for developing Android applications.\n▪ Contributed to developing a Taobao imitation offline app, a Taobao-like mobile application.\n▪ Collaborated with engineers and understood user requirements to develop key features.\n▪ Implemented functional user interface (UI) elements such as login page layouts, interface transitions, and basic functionality for each screen, aligned with the design and requirements for the app.\n▪ Ensured smooth performance and a user-friendly experience by working on UI/UX optimisation.\n▪ Engaged in continuous communication with team members to align development efforts with business goals and contributed to successful app development within the given timeline.\n\n3. ZHIYU Office   (06/2021)   Product Manager\n\n▪ Conducted product research and market analysis, gathered insights on industry trends, customer preferences, and competitor products, and synthesised findings into actionable requirements reports that informed the product development strategy.\n▪ Performed both qualitative and quantitative analysis on user behaviour, utilised tools such as surveys, user interviews, and analytics platforms to gather data, identify user needs, and assess product performance.\n▪ Tracked competitive products and analysed their features, pricing, and positioning in the market, enabled a comprehensive understanding of market dynamics and informing product decisions.\n▪ Developed a full product positioning strategy, outlined key differentiation points, target user segments, and marketing strategies to guide the product’s market introduction and growth.\n▪ Collaborated with cross-functional teams, including marketing, design, and engineering, to ensure product requirements aligned with business goals and customer needs.\n▪ Generated departmental growth strategy reports, leveraged research data and insights to define clear, data-driven action plans for product optimisation and expansion.\n\n\n\n\nEXTRACURRICULUM ACTIVITIES\n\n\n\n\n\n\n\n\nEXTRACURRICULUM ACTIVITIES\n\n\n\n\n\n\nXJTLU Jiangsu Propaganda   (06/2021)   Volunteer\n\n▪ Assisted in distributing pamphlets during admission counselling sessions, providing prospective students and parents with key information about the university.\n▪ Guided parents to counselling seminar rooms, ensuring smooth navigation and a positive experience for attendees.\n▪ Answered inquiries regarding the university’s faculty, academic programs, and prospects, helping prospective students make informed decisions about their academic paths.\n\nXJTLU Running Activity   (05/2021)   Trainee\n\n▪ Participated in the Great Joy Run organised by the university to promote fitness and encourage weight loss, targeting the “Fat Lord” initiative.\n▪ Committed to regular nightly runs under the guidance of an instructor, maintained consistency by attending 3 weeks per month and improved physical fitness and endurance.\n\nGroup Video Culture Promotion   (11/2020)   Subtitle Handler\n\n▪ Organised team members to shoot and integrate promotional videos, ensuring smooth coordination and effective content creation.\n▪ Added video subtitles at precise times, ensuring clarity and accessibility for a broader audience.\n\n\n\n\nSKILLS\n\n\n\n\n\n\n\n\nSKILLS\n\n\n\n\n\n\nLanguage Skills: Mandarin (Native), English (Fluent)\nComputer Skills: Matlab (Proficient), Python (Proficient), R (Proficient), C (Proficient), C++ (Proficient), Java (Proficient)\nEtc.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>CV</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Mr. Zhuoyuan Tang\n\n\nHi, there! My name is Zhuoyuan Tang. I am a student from China. I majored in Data Science and Big Data Technology at Xi ’an Jiaotong-Liverpool University for my undergraduate degree. I am now studying Applied Statistical Modeling and Health Informatics at King’s College London for my master’s degree. My hometown is located in a small city called Dali in Yunnan Province of China. It is very famous for its tourism industry which is a place with pleasant scenery, beautiful natural scenery and suitable for living.\n\n\nThe undergraduate study has made me understand the basic process and knowledge of big data processing, analysis and model construction, as well as the basics of programming language Python, and I have also had some corresponding internship experience. Therefore, I have some basic experience in this field. Now, my master’s program will further expand my coding, data analysis capabilities and apply them to the medical field. In the future, I plan to pursue some related PhD programs after the completion of my master’s degree, so as to further develop my academic research ability and constantly improve myself.\n\n\nI am a undemonstrative but enthusiastic boy. I treat my classmates and teachers with great respect. I also prefer to communicate and listen to others. Usually, I like to travel by bike with my friends, sing songs, play video games and so on. You can learn more about me in this eportfolio (continue to update).",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "SWOT.html",
    "href": "SWOT.html",
    "title": "2  SWOT",
    "section": "",
    "text": "SWOT Analysis\n\n\n\n\n\n\nStrengths\n\n\nBasic knowledge and skills in data science and statistical modeling, good analytical and modeling skills.\nProficient in Python, R and other data analysis tools, with a certain programming ability.\nGood research and problem-solving skills in some certain extent.\nGood listener with others.\nRespect to other’s ideas and work.\n\n\n\n\n\n\n\nWeaknesses\n\n\nRelative introversion, shy to share with others in sometimes.\nMay not suitable for leadership.\nEnglish communication skills are not excellent.\nSome of the high-level machine learning and statistical modeling methods need to be well and further understood.\n\n\n\n\n\n\n\n\n\n\nOpportunities\n\n\nTake more engagement with other classmates or teachers, ask question more frequence with no hesitate if possible.\nLearn new research methods and networking during research project for MSc.\nThe wide application trend of big data in medical, construction, environment and other industries has provided rich application scenarios and opportunities for research.\n\n\n\n\n\n\n\nThreats\n\n\nTake different courses at the same time.\nPractice, especially in coding problems.\nConflict in life and study sometimes.\nThe field of computer and data science is highly competitive both academically and professionally, making it difficult to stand out.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>SWOT</span>"
    ]
  },
  {
    "objectID": "action_plan.html",
    "href": "action_plan.html",
    "title": "3  Action Plan",
    "section": "",
    "text": "1. Leveraging Strengths:\n\n\n\n\n\n\n\nLeveraging Strengths:\n\n\n\n1.1 Strengthen data analysis and programming skills:\n\n\n\n\n\n\nStrengthen data analysis and programming skills\n\n\n\n\n\nI will continue to build my foundation in data science and statistical modeling, utilizing Python, R, and other tools in practical projects. This will enhance my programming capabilities and allow me to accumulate hands-on experience in data analysis and modeling.\n\n\n\n1.2 Active listening and cooperation:\n\n\n\n\n\n\nActive listening and cooperation\n\n\n\n\n\nMaking use of my advantages of being good at listening and respecting the views of others, actively communicate with classmates and teachers, improve teamwork ability, learn from each other in communication, and make up for my shortcomings in advanced skills.\n\n\n\n1.3 Improve research and problem solving ability:\n\n\n\n\n\n\nImprove research and problem solving ability\n\n\n\n\n\nWhen participating in projects and subject research, pay attention to thinking methods of analysis and problem solving, gradually establish systematic research methods and skills, and lay a good foundation for future master’s and doctoral programs.\n\n\n\n\n\n\n2. Seize the Opportunity:\n\n\n\n\n\n\n\nSeize the opportunity:\n\n\n\n2.1 Actively participate in the interaction between teachers and students:\n\n\n\n\n\n\nActively participate in the interaction between teachers and students\n\n\n\n\n\nAsking questions from teachers and classmates in class or after class, and maintain an active learning attitude. In particular, he actively participates in discussions and collaborations in research projects to expand his research methods and academic contacts.\n\n\n\n2.2 Pay attention to the application of big data industry:\n\n\n\n\n\n\nPay attention to the application of big data industry\n\n\n\n\n\nCombine the application trend of big data in the medical, construction and environment industries, make good use of the opportunities in the research project, combine the knowledge with practical application scenarios, and improve their application ability and employment competitiveness.\n\n\n\n2.3 Develop skills through research projects:\n\n\n\n\n\n\nDevelop skills through research projects\n\n\n\n\n\nConsciously learn new research methods and tools in the master’s program to expand my knowledge and prepare for future career development.\n\n\n\n\n\n\n3. Improve the Weakness:\n\n\n\n\n\n\n\nImprove the weakness:\n\n\n\n3.1 Overcome introversion and improve communication skills:\n\n\n\n\n\n\nOvercome introversion and improve communication skills\n\n\n\n\n\nGradually develop confidence in sharing ideas in a team, try to express ideas proactively in group discussions or classes, gradually overcome the psychology of shyness to share, and improve communication and expression skills.\n\n\n\n3.2 Deepen technical knowledge:\n\n\n\n\n\n\nDeepen technical knowledge\n\n\n\n\n\nLearn advanced machine learning and statistical modeling methods in a planned way, especially for the technical knowledge that may be required for master’s research, and gradually deepen understanding through additional online courses or textbooks.\n\n\n\n\n\n\n4. Respond to Threats:\n\n\n\n\n\n\n\nRespond to threats:\n\n\n\n4.1 Reasonable arrangement of study and life:\n\n\n\n\n\n\nReasonable arrangement of study and life\n\n\n\n\n\nMake a detailed study and life plan, prioritize urgent tasks, and avoid task accumulation. Improve learning efficiency through time management to avoid conflicts between courses affecting progress.\n\n\n\n4.2 Increase practical practice:\n\n\n\n\n\n\nIncrease practical practice\n\n\n\n\n\nIncrease practical opportunities in programming and data analysis, especially for possible code problems, to solve in advance in daily practice to improve the ability to deal with problems.\n\n\n\n4.3 Balance the pressure of study and life:\n\n\n\n\n\n\nBalance the pressure of study and life\n\n\n\n\n\nCultivate healthy living habits, arrange time reasonably, and ensure that you maintain a good physical and mental state in the busy study life, in order to better cope with the pressure of study and life.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Action Plan</span>"
    ]
  },
  {
    "objectID": "reflective_model.html",
    "href": "reflective_model.html",
    "title": "4  Reflective_model",
    "section": "",
    "text": "STARR Model Reflection\n\nI selected the STARR model for my ePortfolio reflection due to its clear, structured approach that facilitates a comprehensive and systematic review of experiences and growth. The five-step process—Situation, Task, Action, Result, and Reflection—enables me to deeply analyze each phase of my experiences, thereby identifying areas for improvement more effectively. Compared to other reflective models, STARR offers a targeted framework that encourages critical thinking at every stage, making it especially suitable for tracking my development throughout the MSc program.\n\n1. Situation2. Task3. Action4. Results5. Reflection\n\n\n\nI am naturally introverted and have often felt hesitant in social and academic communication, though I am capable of solving problems independently and often draw on others’ insights and methods when approaching tasks.\n\n\n\n\nMy primary objectives are to successfully complete the MSc programme, strengthen my coding and data analysis capabilities, improve my communication skills, and build greater confidence both academically and professionally.\n\n\n\n\nI plan to engage more proactively in classes and group projects, seek out innovative learning methods, and consistently practice programming and communication skills to enhance my overall learning efficiency and competencies.\n\n\n\n\nBy actively participating in academic and collaborative activities, I aim to overcome communication barriers, acquire advanced technical and programming skills, and gain confidence in team-based and research-oriented environments.\n\n\n\n\nThis reflective process has underscored the value of communication and collaboration. Although I began with limited confidence in expressing myself, consistent practice and active engagement have markedly improved my ability to share ideas and work within teams. Furthermore, exposure to innovative methods and deeper analytical approaches during projects has not only enriched my learning experience but also honed my problem-solving skills. This journey has reinforced my belief that proactive participation and deliberate practice are vital to personal and professional growth, and has motivated me to maintain this mindset throughout my MSc and future career.\n\n\n\n\n\n\nReflection on SWOT, Action Plan, and Transition to the MSc\n\n\n\n\n\n\n\nReflection on SWOT, Action Plan, and Transition to the MSc\n\n\n\n\n\n\nConducting a SWOT analysis and developing a subsequent action plan has allowed me to gain a clearer understanding of my strengths, weaknesses, and future direction. These exercises, informed by my undergraduate achievements and experiences, helped me inventory my existing skills, identify competency gaps, and recognize both opportunities and challenges in my academic and professional journey. As I transition into the MSc programme, these plans will serve as a roadmap, enabling me to systematically improve my programming and communication skills, thereby laying a solid foundation for research projects and long-term development.\n\n\nThe SWOT analysis was particularly revealing: it helped me recognize that my introversion could hinder communication and collaboration, but also highlighted my aptitude for data science and strong motivation for learning and growth. As an international student, studying abroad presents a significant opportunity to develop my communication skills and adapt to a new academic culture. My action plan outlines concrete steps—such as increasing class participation, enhancing English proficiency, and dedicating time to programming practice—that will help me convert these insights into tangible progress, thereby boosting my confidence in tackling these challenges.\n\n\nIn my ePortfolio, I intend to demonstrate my readiness and enthusiasm for the MSc project—and ultimately, a PhD or research career—by aligning my goals with specific, actionable plans for growth. This process of reflection has not only deepened my self-awareness but also helped me articulate my learning motivations and professional expectations more clearly. By doing so, I aim to present a coherent narrative of personal and academic development, illustrating my preparedness for the MSc programme and my commitment to future achievements.\n\n\n\n\n\n\nMSc Modules\n\n\nTerm (Semester) 1 ModulesTerm (Semester) 2 ModulesTerm (Semester) 3 Modules\n\n\n\n\nIntroduction to Statistical Programming   (7PAVITPR)\nIntroduction to Statistical Modelling   (7PAVITSM)\nIntroduction to Health Informatics   (7PAVITHI)\n\n\n\n\n\n\n\n\nReflection on Semester 1\n\n\n\n\n\n▪ Statistical programming enables me to master the basic knowledge of some programming languages, improve the ability to use programming tools (R and Python) for data processing and analysis, and lay the foundation for subsequent modeling and health data analysis. Below are the basic statistical distribution and visualization methods that I have learned in this course:\n\n\n\n\n\n\nInstance for Statistical programming\n\n\n\n\n\nFor R section:\n\nlibrary(here)\nlibrary(haven)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(moments)\nlibrary(e1071)\n\n#Plot histogram of age & age for different countries.\n#par(mfrow = c(1, 2)) used for based plot on single figure\nhist_age &lt;- new_df |&gt;\ndrop_na(age) |&gt; #exclude missing values\nggplot() +\naes(x= age) +\ngeom_histogram(binwidth = 0.8, color = \"black\", fill = \"purple\") + #Set color and other styles free\nlabs(title = \"Histogram of Age\", x= \"Age\", y= \"Count (Frequency)\") +\ntheme_minimal()\nhist_age\n\nhist_age_countries &lt;- new_df |&gt;\n  drop_na(age) |&gt; # exclude missing values\n  ggplot() +\n  aes(x = age, fill = cntry) +\n  geom_histogram(binwidth = 0.8) +\n  labs(title = \"Histogram of Age for Different Countries\", x = \"Age\", y = \"Count (Frequency)\") +\n  facet_wrap(~ cntry, scales = \"free\") +    # Subplot by using facet_wrap, and set scales free for x and y axis.\n  theme_minimal()\n\nhist_age_countries\n\n \nFor Python section:\n\n# Import library\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Plot scatter, show relationship between Height and Weight for different category\n\n# Plot\ncategory_values = [0, 1, 2, 3]\ncolors = ['lightblue', 'green', 'purple', 'orange']\nlabels = ['0: underweight', '1: healthy weight', '2: overweight', '3: obese']\n\n# Use loop instead of one by one\nfor i in range(4):\n    x = df_preprocess[df_preprocess[\"Category\"] == i][\"Height\"]\n    y = df_preprocess[df_preprocess[\"Category\"] == i][\"Weight\"]\n    plt.scatter(x, y, alpha = 0.8, color = colors[i], label = labels[i])  # Set different label and colour\n    \n    slope, intercept = np.polyfit(x, y, 1)\n    plt.plot(x, slope * x + intercept, color = colors[i], label = labels[i]) # Draw the regression line\n  \n# Title and label\nplt.title(\"Height vs Weight Scatter Plot (for Category)\")\nplt.xlabel('Height (cm)')\nplt.ylabel('Weight (kg)')\nplt.xticks(rotation = 45)  # Rotate the X-axis label\n\n# Add legend for categorise\nplt.legend(title='BMI Category', loc = 'best', bbox_to_anchor=(1, 0.75))\n\n# Presentation of the graph\nplt.show()\n\n (Based on the result from this scatter plot for different categories, we can observe that if we keep the Height constant, the higher the value of Weight, the more likely the category is obese, followed by overweight, healthy weight, underweight. According to the regression line for each variable, under the same category condition, Height and Weight show a certain positive relationship.)\n\n# Count the number of people in each BMI category for each gender\n\n# size() calculates the number of people in each BMI category for each gender and \n# converts the results into a DataFrame format using the unstack() method\ncategory_gender_counts = df_preprocess.groupby(['Gender', 'Category']).size().unstack(fill_value=0)\n\n# Draw the bar chart\nplot_bar = category_gender_counts.plot(kind='bar', stacked=False, color=['skyblue', 'lightgreen', 'purple', 'orange'])\n\n# Set the title and label\nplt.title('Number of People in Each BMI Category by Different Gender')\nplt.xlabel('Gender')\nplt.ylabel('Number of People')\nplt.xticks(rotation = 45)  # Rotate the X-axis label\n\n# Add categories name\nbmi_labels = ['0: underweight', '1: healthy weight', '2: overweight', '3: obese']\n\nplt.legend(bmi_labels, title='BMI Category', loc='best', bbox_to_anchor=(1, 0.6))  # Add legend\nplt.grid(axis='y')  # Add gridlines\nplt.tight_layout()  # Automaticly adjust layout \n\n# Display the value next to the bar\nfor container in plot_bar.containers:\n    plot_bar.bar_label(container, label_type='edge')  # label_type='edge' Place the label above or next to the bar\n\n\nplt.show()\n\n (According to the results of the bar chart, the number of obese people are both highest in both Female and Male, with 167 and 165 obese women and men respectively. However, underweight is the least, with the number of 13 and 21 respectively. From this, we can conclude that obese people are the main situation, and some relevant measures should be taken to reduce this phenomenon.)\n\n\n\n▪ By learning statistical modelling methods, I deepened my understanding of data analysis, causation, and inference, making me more skilled in selecting and evaluating suitable statistical models for data such as significance, confidence intervals, etc.\n\n\n\n\n\n\nInstance for Statistical modelling\n\n\n\n\n\n\n\n\nDAG Causal Mediation Diagram with Causal Paths\n\n\n\n# Table 1 combine all samples together\n# Create a new column that indicate whether belong complete case or not\npollution_data &lt;- pollution_data |&gt;\n  mutate(complete_case = ifelse(complete.cases(pollution_data), \n                                \"Analytical\", \"Excluded\"))\n\nFull_table1 &lt;- pollution_data |&gt;\n  tbl_summary(include = c(-pid),\n              by = complete_case,   # Separate by whether complete case\n              statistic = list(all_continuous() ~ \n                                 \"{mean} ({sd}) {median} ({p25}, {p75})\",\n                               all_categorical() ~ \"{n} ({p}%)\"),\n              missing_text = \"(Missing)\") |&gt;\n  modify_caption(\"Full Descriptive statistics ('Table 1')\") |&gt;    # Add table's title\n  add_overall()     # Include overall statistics\n\nFull_table1\n\n Instance of statistical modelling:\n\n# Use interaction effect without considering the variation of borough\n\nm1 &lt;- lm(phq9 ~ pm25 * smoker + borough + employment, data = analytical_sample)\n\n#m1 &lt;- lm(phq9 ~ pm25 * smoker + borough + age + ses + employment + gender + exercise, data = analytical_sample)\n\nsummary(m1)\n\n# executive outcome\nFixed effects:\n                  Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)        5.03689    0.93420 496.55124   5.392 1.08e-07 ***\npm25               0.22120    0.05517 819.30050   4.010 6.64e-05 ***\nemploymentOffice  -1.05971    0.42784 823.74867  -2.477   0.0135 *  \nsmokerSmoker       5.70257    0.35020 815.59065  16.284  &lt; 2e-16 ***\n\n\n\n\n▪ The core concepts of health informatics, including electronic health records (EHRs), data standards, and privacy protection issues, gave me a more complete understanding of the challenges of healthcare data management and analysis.\n\n\n\n\n\n\nInstance for health informatics\n\n\n\n\n\nDistributions of the values across variables and outcome classes\n\n# Visualize the distribution in different Mortality categories for each variable\n\nvariable_columns = [\"MeanBP\", \"DiasBP\", \"SysBP\", \"Centralvenouspressure \", \"Creatinine\", \"DailyWeight\", \n                    \"Glucose\", \"HeartRate\", \"Haemoglobin\", \"Platelets\", \"SpO2\", \"Temperature C\"]\n\nfig, axs = plt.subplots(3, 4, tight_layout=True)\nfig.set_size_inches(20,12)\naxs = axs.ravel()   # Ensure that axs is a one-dimensional array\n\nfor index, variable in enumerate(variable_columns):\n    axs[index].hist(data_timeseries[data_timeseries[\"Mortality\"] == 0][variable].dropna(), \n                    label=\"Live (Mortality: 0)\", bins=10, alpha=0.75, color=\"green\")\n    axs[index].hist(data_timeseries[data_timeseries[\"Mortality\"] == 1][variable].dropna(), \n                    label=\"Death (Mortality: 1)\", bins=10, alpha=0.75, color=\"red\")\n\n    axs[index].legend(loc='best')\n    axs[index].set_title(f\"Distribution of {variable} across outcome\")\n    axs[index].set_xlabel(variable)\n    axs[index].set_ylabel(\"frequency\")\n    axs[index].grid(axis='y')\n\nplt.tight_layout()\nplt.show()\n\n\nMissing Extent\n\n# Visualise the missing extent for each variable\nimport missingno as msno\n\nmsno.bar(data_timeseries)\nplt.show()\n\n Visualise changes in Mean Arterial Pressure (MAP) over time for each patient\n\n# Visualise changes in MAP over time for each patient\n\nfig, axes  = plt.subplots(2, n_cols, figsize = (20, 10))\nflt = axes.flatten()\nindex=0\nfor id in patient_ids:\n    subset = data_timeseries.loc[data_timeseries['PatientID']==id]\n    \n    # Change \"plot\" to \"step\" if you want to achieve the 'stepped' look such that for each point\n    # which is mentioned in our practical exercises\n    flt[index].plot(subset[\"Hour\"], subset[\"MAP\"], color='green', linestyle='dashed', marker='o', label='MAP')\n    \n    # Set the threshold line (65 mmHg)\n    flt[index].axhline(65, color='purple', label='Threshold (65 mmHg)')\n    \n    # Highlight all point that may on vasopressors\n    flt[index].scatter(subset.Hour[subset[\"MAP\"] &lt; 65],\n                       subset.MAP[subset[\"MAP\"] &lt; 65],\n                       color = \"blue\", label=\"Vasopressors use\", s = 50, zorder = 2) # \"s\" is sizes, \"zorder\" Represents the order of plotting\n\n    # Draw and highlight the first hour at which the patient was started on vasopressors\n    if not subset.Hour[subset[\"MAP\"] &lt; 65].empty and not subset.MAP[subset[\"MAP\"] &lt; 65].empty:\n        flt[index].scatter(subset.Hour[subset[\"MAP\"] &lt; 65].min(),\n                           subset.MAP[subset[\"MAP\"] &lt; 65].iloc[0],\n                           color = \"red\", label=\"First vasopressors use\", s = 100, zorder = 3)\n        \n        # Display the corresponding values in the graph\n        flt[index].annotate(f\"Hour: {subset.Hour[subset[\"MAP\"] &lt; 65].min()}\\nMAP: {subset.MAP[subset[\"MAP\"] &lt; 65].iloc[0]: .2f}\",\n                           (subset.Hour[subset[\"MAP\"] &lt; 65].min(), subset.MAP[subset[\"MAP\"] &lt; 65].iloc[0]))\n    \n    flt[index].set_title(f\"Patient ID: {id}\")\n    flt[index].set_xlabel(\"Hour\")\n    flt[index].set_ylabel(\"MAP (mmHg)\")\n    flt[index].legend(loc = \"best\")\n    index=index+1\n    if index == 10:\n        break;\n\nplt.tight_layout()\nplt.show()\n\n For example, the patient who’s id is 171456, I have given the MAP variation lines (represented by green), threshold value lines (65mmHg, dashed purple), as well as all possible vasopressors applied (blue dots) and points that may apply vasopressors for the first time (red dots), and shown their corresponding values in the graph. In this patient’s case, The first time vasopressors was applied was one hour after admission, and the MAP value at that time was 56.67. The content of the visualizations is consistent with other patients.\n\n\n\n\n\n\n\n\n\n\nMultilevel and Longitudinal Modelling   (7PAVMALM)\nPrediction Modelling   (7PAVPRMD)\nMachine learning for Health and Bioinformatics   (7PAVMALE)   As an auditor\n\n\n\n\n\n\n\n\nReflection on Semester 2\n\n\n\n\n\n▪ The multimodal and longitudinal modeling courses have enabled me to master the core methods of multi-level and longitudinal data modeling, and enhanced my analytical ability in processing complex structured data with stata programming language, especially in the application of health research.\n\n\n\n\n\n\nInstance for multimodal and longitudinal modeling\n\n\n\n\n\nLongitudinal Profiles\n\n* Record history and log files\ncapture log close\nlog using analysis.log, replace\n\n* Load dataset\nuse \"Socrates_assignment dataset.dta\", clear\n\n// Calculate group mean\npreserve\ncollapse (mean) mean_panss = panss, by(interven time)\n\n// Graphical display\ntwoway connected mean_panss time if interven==1, sort lcolor(blue) || ///\n       connected mean_panss time if interven==0, sort lcolor(red)    ///\n       xlabel(0 \"Baseline\" 1 \"6 weeks\" 2 \"3 months\" 3 \"9 months\" 4 \"18 months\") ///\n       legend(pos(6) order(1 \"Intervention\" 2 \"Control\"))           ///\n       ytitle(\"Mean PANSS\") title(\"Longitudinal Profiles of the Mean PANSS\") \nrestore\n\n\n\n\nLongitudinal Profiles of the Mean PANSS Measure for Control and Intervention Arm\n\n\nKaplan-Meier survival curve analysis for two groups  Kaplan-Meier results show that the survival curve of the intervention group is always lower than that of the control group, indicating that the intervention group has a higher dropout rate, and the difference between the two groups gradually expands over time.\nMultilevel modelling Stata syntax instance\n\nmixed panss i.interven##c.time panss0 sex ageentr || centre: , mle\n\nmixed panss i.interven##c.time panss0 sex ageentr || therapis: , mle\n\nmixed panss interven##c.time panss0 sex ageentr || centre: || therapis: , mle\n\nmixed panss interven##c.time panss0 sex ageentr || centre: || therapis: ||\nidnumber: , mle\n\n# etc.\n\n\n\n\n▪ By learning predictive modeling techniques, I have deepened my understanding of the application of machine learning and statistical models in health data analysis, the difference between statistical models and predictive models, and improved my ability of model evaluation and optimization.\n\n\n\n\n\n\nInstance for predictive modeling\n\n\n\n\n\nDescriptive Statistics for US and International Trials \nLasso regression\n\n### Lasso\n\n# 1) define the number of repetitions (to repeat all nested CV several\n# times using different splits of the data)\ncv_repetitions = 10 \n\n# 2) number of outer CV folds (to test the best lasso model on unseen data)\nouter_cv = 5\n\n# 3) number of inner CV folds (used to find the best lambda)\ninner_cv = 10\n\n# 4) number of repeats of CV inner loop to identify best lambda\nrepeats = 10\n\n# This is the utmost outer loop for repetitions: \nfor (rep_n in seq(1:cv_repetitions)){\n  set.seed(456+rep_n)\n  print(paste0(\"Current nested CV repetition: \",rep_n))\n  folds_nestedcv &lt;- createFolds(y, list = FALSE, k=outer_cv )\n  table(folds_nestedcv)\n  dim(x[folds_nestedcv==1,])\n  \n  # This is the outer CV loop\n  for (i in seq(1:outer_cv)){\n    print(i)\n    x_test = x[folds_nestedcv == i, ]\n    y_test = y[folds_nestedcv==i]\n    x_train = x[folds_nestedcv != i , ]\n    y_train = y[folds_nestedcv !=i]\n   \n    #Inner CV loop we outsource from cv.glmnet with inner_cv number of folds\n    # we only use train sets:\n    \n    # Important for repeated CV using caret, we do not need to separate x and y in different objects. We need to combine x and y into a dataframe \n    # This is a bit clunky. y needs to stay as a factor. Therefore, the as.data.frame(y_train) \n    # in front of it\n    traindata&lt;-cbind(as.data.frame(y_train),x_train)\n\n    ### Set up training settings object\n   \n    trControl &lt;- trainControl(method = \"repeatedcv\", # repeated CV \n                              repeats = repeats,          # number of repeated CV\n                              number = inner_cv   ,         # Number of folds\n                              summaryFunction = twoClassSummary,  #function to compute performance metrics across resamples.AUC for binary outcomes\n                              classProbs = TRUE, \n                              savePredictions = \"all\",\n                              allowParallel = TRUE,\n                              selectionFunction = \"best\" ) # best - minimum lambda, oneSE for minimum lambda + 1 Se, Tolerancwe for minimum + 3%\n\n    \n    ### Set up grid of parameters (lambdas) to test.\n    # It is better to provide glmnet a user-defined grid of lambdas! \n    # We have to set alpha to either 0 (Ridge) or 1 (Lasso)\n    # Later we will see that we can also have alphas ranging between 0 and 1 (Elastic net)\n    \n    # Set up grid of parameters to test\n    Grid_lasso = expand.grid(alpha=c(1),   # L1 & L2 mixing parameter\n                       lambda=2^seq(4,-10, by=-0.1)) # regularization parameter lambda\n    # Grid  # lambda ranges from 0.001 to 4, this often works well. \n    # You can have smaller units by changing b= -0.1 to by = -0.01\n    # Too make your code faster its better to establish the grid outside the loop\n    \n    ### Run training over tuneGrid and select best model\n    set.seed(456)\n    cl=makeCluster(4);registerDoParallel(cl)   # using more than one core\n    \n    cv10_lasso &lt;- train(y_train ~ .,             # model formula (. means all features)\n                        data = traindata,         # data.frame containing training set\n                        method = \"glmnet\",     # model to use\n                        metric =\"ROC\",         # Optimizes AUC, best with deviance for unbalanced outcomes \n                        family=\"binomial\",     # logistic regression\n                        trControl = trControl, # set training settings\n                        tuneGrid = Grid_lasso)     # set grid of paramameters to test over, if not specified defualt gris is used (not always the best)\n\n    stopCluster(cl)  # stop using cores\n    \n    \n    \n    #######    \n    \n    \n    # Save best alpha and lambda\n    best_alpha_lasso &lt;-get_best_result(cv10_lasso)$alpha  # alpha is 1 for lasso\n    best_lambda_lasso &lt;- get_best_result(cv10_lasso)$lambda\n    \n    \n    # now we will use the best model fitted to \n    # the train data to predict test outcomes:\n    \n    # Predict outcomes in the whole training set, \n    # Comment: newx is here x_train \n    predict_train_lasso&lt;-predict(cv10_lasso$finalModel,type=\"response\",newx=x_train, s=best_lambda_lasso, alpha=best_alpha_lasso)\n    \n    \n    # now we will use the best model fitted to \n    # the train data to predict test outcomes using x_test:\n    predict_test_lasso&lt;-predict(cv10_lasso$finalModel,type=\"response\",newx=x_test, s=best_lambda_lasso, alpha=best_alpha_lasso)\n    \n  }    #end of the outer loop\n  \n  \n}    #end of repeated outer loop\n\n# Let's check the resulting measures:\n# Matrix\napparent_lasso &lt;- calculate_metrics((predict_train_lasso[,1]), traindata[,1])\n\nprint(apparent_lasso)\n\n      AUC  Accuracy Sensitivity Specificity       PPV       NPV\nAccuracy 0.8328734 0.7572115 0.5664557 0.874031 0.7336066 0.7670068\n\n# Plot regularization paths for the best model\nplot(cv10_lasso$finalModel, xvar=\"lambda\", label=T)\n\n\n\n\nRegularization paths for the best model of lasso regression\n\n\nCalibration plot\n\n# Use val.prob.ci.2 to calculate calibration performance and plot\ncalPerf &lt;- val.prob.ci.2(pred_prob_external[, 1], y_external_binary, \n                         main = \"Calibration Plot (External Validation)\")\n\n (For calibration, the calibration slope is 1.21 with 95% confidence interval [1.05, 1.37], which slightly greater than 1 indicates that the model may slightly overestimate the risk of high-risk individuals or underestimate the risk of low-risk individuals in the external data. Calibration intercept is 0.12 with 95% confidence interval [-0.01, 0.25], which close to 0 and confidence interval contains 0, indicating a small overall prediction deviation. The predicted probability is basically consistent with the actual observed risk, but there exists a slight overestimation trend in the latter half of the high-risk region.)\n\n\n\n▪ I did not enroll the course machine learning in Health and Bioinformatics, but I applied as an auditor, which also have some inspiration for my research project. Includes an introduction to some widely used machine learning models, such as unsupervised, supervised learning, and neural networks, etc. Design a complete machine learning pipeline solution, covering data preprocessing, variable selection, and optimization evaluation of algorithm execution, etc.\n\n\n\n\n\n\n\nArtificial Intelligence for Healthcare Analytics   (7PAVAIHA)\nCausal Modelling and Evaluation   (7PAVCIAE)\nNatural Language Processing   (7PAVNLPS)\n\n\n\n\n\n\n\n\nReflection on Semester 3\n\n\n\n\n\n▪ The artificial intelligence module has once again enabled me to solidify the introduction, usage examples of common algorithms such as genetics and searching, as well as machine learning, deep learning, and reinforcement learning. Finally, I learned about a very significant field - Explainable AI (XAI). This is a particularly important aspect for both model algorithm interpretation and reliability analysis.\n\n\n\n\n\n\nInstance for Artificial Intelligence\n\n\n\n\n\nExample of neural network construction and training\n\n# Divide the data for label\nTrain_features = df_train.drop(['ID', 'Words (N)','age', 'gender', 'mmse', 'label'], axis=1)\nTrain_label = df_train['label']\n\nX_train, X_val, y_train, y_val = train_test_split(Train_features, Train_label, test_size = 0.2, random_state = 42)\n\n# Reshape data into LSTM format (samples, timesteps, features)\nX_train_lstm = np.expand_dims(X_train.values, axis=1)\nX_val_lstm = np.expand_dims(X_val.values, axis=1)\n\n\n# Create the model\ndef build_bilstm_GNA_model(lstm_params=64, dense_params=32, dropout_rate=0.3, \n                       optimizer='adam', learning_rate=0.001, activation='relu', noise_gna=0.1):\n    model = Sequential()\n    # Add Gaussian noise layer to simulate data augmentation\n    model.add(GaussianNoise(stddev=noise_gna, input_shape=(1, X_train.shape[1])))\n    model.add(Bidirectional(LSTM(lstm_params, return_sequences=False)))\n    model.add(BatchNormalization())\n    model.add(Dropout(dropout_rate))\n    \n    model.add(Dense(dense_params, activation=activation))\n    model.add(BatchNormalization())\n    model.add(Dropout(dropout_rate))\n    \n    model.add(Dense(1, activation='sigmoid'))\n    \n    if optimizer == 'adam':\n        opt = Adam(learning_rate=learning_rate)\n    else:\n        opt = SGD(learning_rate=learning_rate)\n        \n    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\n# Wrap with KerasClassifier\nkeras_bilstm_gna_classification = KerasClassifier(\n    model=build_bilstm_GNA_model,\n    epochs=50,\n    batch_size=32,\n    verbose=0\n)\n\n# Define hyperparameter grid\nparam_grid_bilstm_gna = {\n    'model__lstm_params': [64, 128],\n    'model__dense_params': [32, 64],\n    'model__dropout_rate': [0.2, 0.3, 0.5],\n    'model__optimizer': ['adam', 'SGD'],\n    'model__learning_rate': [0.001, 0.01],\n    'model__activation': ['relu', 'tanh'],\n    'model__noise_gna': [0.1], # Different intensities of Gaussian noise, Here, we have only set one value due to time and memory constraints.\n    'batch_size': [32, 64]\n}\n\n# Grid search with stratified K-fold\ngrid_bilstm_gna = GridSearchCV(\n    estimator=keras_bilstm_gna_classification,\n    param_grid=param_grid_bilstm_gna,\n    cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),\n    scoring='accuracy',\n    verbose=0\n)\n\n# Fit grid search\ngrid_bilstm_gna.fit(X_train_lstm, y_train)\n\n# Output best parameters\nprint(\"Best parameters for BiLSTM with Gaussian Noise Augmentation classification: \\n\", grid_bilstm_gna.best_params_)\nprint(\"Best cross-validation accuracy: {:.3f}\".format(grid_bilstm_gna.best_score_))\n\n# Filter and format best parameters\nallowed_keys_bilstm_gna = ['lstm_params', 'dense_params', 'dropout_rate', 'optimizer', 'learning_rate', 'activation', 'noise_gna']\nbest_bilstm_gna_params = {k.replace('model__', ''): v for k, v in grid_bilstm_gna.best_params_.items() if k.startswith('model__') and k.replace('model__', '') in allowed_keys_bilstm_gna}\n\n# Rebuild model\nprint(\"\\nTraining best BiLSTM with Gaussian Noise Augmentation classification model:\\n\")\nbest_bilstm_gna_model = build_bilstm_GNA_model(**best_bilstm_gna_params)\n\n# Train with early stopping\nearly_stopping = EarlyStopping(\n    monitor='val_loss',\n    patience=25,\n    min_delta=0.001,\n    mode='auto',\n    verbose=1,\n    restore_best_weights=True\n)\n\nhistory_bilstm_gna = best_bilstm_gna_model.fit(\n    X_train_lstm, y_train,\n    epochs=100,\n    batch_size=grid_bilstm_gna.best_params_['batch_size'] if 'batch_size' in grid_bilstm_gna.best_params_ else 32,\n    validation_data=(X_val_lstm, y_val),\n    callbacks=[early_stopping],\n    verbose=1\n)\n\nplot_learningCurve(history_bilstm_gna)\n\n# Visualise the best model\nprint(best_bilstm_gna_model.summary())\n\nSVG(model_to_dot(best_bilstm_gna_model, show_shapes=True, show_layer_names=True, dpi=65).create(prog='dot', format='svg'))\n\n\n\n# Evaluation\ny_prob = best_bilstm_gna_model.predict(X_val_lstm)\n# Binaryization processing: Convert probabilities into labels\ny_pred = (y_prob &gt;= 0.5).astype(int)\nclassification_evaluate(y_val, y_pred)\n\n# Save model for more convenient\nbest_bilstm_gna_model.save('bilstm_gna_AD_classification_model.h5')\n# model = keras.models.load_model('bilstm_gna_AD_classification_model.h5')\n\n \u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step\nClassifier Accuracy: 0.818\nF1 Score: 0.833\n          precision    recall  f1-score   support\n\n       0       0.80      0.80      0.80        10\n       1       0.83      0.83      0.83        12\n\naccuracy                           0.82        22\nmacro avg 0.82 0.82 0.82 22\nweighted avg 0.82 0.82 0.82 22\nSHAP Interpreter–Explainable AI (XAI)\n\n# Construct SHAP explainer\nexplainer = shap.Explainer(baseline_AD_classification_model, X_val, feature_perturbation='interventional')\nshap_values = explainer(X_val)\n\n# Overall Explanation Visualisation\nplt.title(\"Overall Feature Importance by SHAP\")\nshap.summary_plot(shap_values, X_val)\n\n# Define groups\ngroups = [\n    ('Male', df_val[df_val['gender'] == ' male ']), \n    ('Female', df_val[df_val['gender'] == ' female ']),\n    ('Younger', df_val[df_val['age'] &lt; 70]),\n    ('Older', df_val[df_val['age'] &gt;= 70])\n]\n\n# Set subplot\nfig, axes = plt.subplots(nrows=2, ncols=2)\naxes = axes.flatten()\n\n# SHAP plot by different groups\nfor i, (name, group) in enumerate(groups):\n    group_X = X_val.loc[group.index]\n    shap_values = explainer(group_X)\n    \n    plt.sca(axes[i])\n    shap.summary_plot(shap_values.values, group_X, show=False)\n    axes[i].set_title(name)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n▪ The Causal Modeling and Evaluation course has enabled me to deepen my understanding and application of the Stata programming language (State-of-the-art libraries such as pscore, mediate, etc.). This further deepened the understanding of the methods of observational experimental research, including baseline inference, methods for handling observed confounding and not observed (such as instrumental variables estimation, Propensity scoring, etc.) as well as mediation analysis, etc. These causal assessment analyses of clinical experiments are all of great significance.\n\n\n\n\n\n\nInstance for Causal Modeling\n\n\n\n\n\nDAG for Mediation/Confounder  \nPropensity scores and the region of common support\n\n* Generate propensity scores and characterise the region of common support\n* check relationship between exercise_treatment and other predictors \ntwoway (lowess exercise_treatment base_BMI)\ntwoway (lowess exercise_treatment age)\ntwoway (lowess exercise_treatment hypo_thyroid)\n\n* Generate a predicted propensity score from a logistic regression on the three covariates.\npscore exercise_treatment base_BMI age hypo_thyroid, pscore(mypscore1) blockid(myblock1) comsup numblo(5) logit\n\n* Check adequacy of fitted relationship\ngraph twoway (lowess exercise_treatment base_BMI) (lowess mypscore1 base_BMI), ///\n    title(\"Relationship: Base_BMI vs Exercise Treatment\") \ngraph twoway (lowess exercise_treatment age) (lowess mypscore1 age), ///\n    title(\"Relationship: Age vs Exercise Treatment\") \ngraph twoway (lowess exercise_treatment hypo_thyroid) (lowess mypscore1 hypo_thyroid), ///\n    title(\"Relationship: Hypo_thyroid vs Exercise Treatment\") \n\n\nSome other stata code\n\n* Check that the propensity score strata balance the cohorts\n* standardised difference &lt;0.025\nxi: pbalchk exercise_treatment base_BMI age hypo_thyroid if comsup==1, strata(myblock1)\n\n* iii. an estimator derived from a stratified propensity score analysis (bootstrapping (1000 replications, percentile method))\natts BMI exercise_treatment, pscore(mypscore1) blockid(myblock1) comsup boot reps(1000)\n\n\n* iv. a nearest neighbour propensity score matching estimator (with replacement) (bootstrapping (1000 replications, percentile method))\nbs \"psmatch2 exercise_treatment mypscore1 if comsup==1, neighbor(1) outcome(BMI)\" \"r(att)\", rep(1000)\npsmatch2 exercise_treatment mypscore1 if comsup==1, neighbor(1) outcome(BMI)\nxi: pbalchk exercise_treatment base_BMI age hypo_thyroid if comsup==1, wt(_weight)\n\n* v. a radius propensity score matching estimator (with radius=0.1 SD, with replacement) (bootstrapping (1000 replications, percentile method))\nbs \"psmatch2 exercise_treatment mypscore1 if comsup==1, radius caliper(0.1) outcome(BMI)\" \"r(att)\", rep(1000)\npsmatch2 exercise_treatment mypscore1 if comsup==1, radius caliper(0.1) outcome(BMI)\nxi: pbalchk exercise_treatment base_BMI age hypo_thyroid if comsup==1, wt(_weight)\n\n* vi. a kernel propensity score matching estimator (with replacement) (bootstrapping (1000 replications, percentile method))\nbs \"psmatch2 exercise_treatment mypscore1 if comsup==1, kernel outcome(BMI)\" \"r(att)\", rep(1000)\npsmatch2 exercise_treatment mypscore1 if comsup==1, kernel outcome(BMI)\nxi: pbalchk exercise_treatment base_BMI age hypo_thyroid if comsup==1, wt(_weight)\n\n* --------------------------------------------------------------------------------------------------\n\n* Generate IPTW weights\ngen IPTW_exp = 1/mypscore1\ngen IPTW_con = 1/(1-mypscore1)\ngen IPTW = prog_receipt*IPTW_exp + (1-prog_receipt)*IPTW_con\n\n* --------------------------------------------------------------------------------------------------\n*** Gformula with 10,000 simulations\ngformula funct_follow group fear_post funct_base fear_base funct_mid, mediation /*\n*/ out(funct_follow) ex(group) mediator(fear_post) base_confs(funct_base fear_base) post_confs(funct_mid) /*\n*/ eq(funct_mid: group funct_base fear_base, /*\n*/ fear_post: group funct_base fear_base funct_mid, funct_follow: group fear_post funct_base fear_base funct_mid) /*\n*/ com(funct_mid:regress, fear_post:regress, funct_follow:regress) control(fear_post:0) seed(123) samples(10000) obe\n\n\n\n\n▪ In the natural language processing module, many methods are described for extracting and processing the unstructured text of electronic health records in medicine. I learned many representation methods such as part-of-speech, vectorization etc., and combined various machine learning, deep learning, and transformer algorithms to complete applications such as text classification. I also practiced fine-tuning some pre-trained models on Hugging face and applied large language models, which was very enriching and fulfilling for me.\n\n\n\n\n\n\nInstance for NLP\n\n\n\n\n\nPoster for NLP Assignment: Classification of PubMed RCT corpus \n\n\n\n\n\n\n\n\n\n\n\nResearch Project\n\nThe Effect of Masking Strategies on Imputation and Prediction Quality in Clinical EHR Data\nStudent: Zhuoyuan Tang\nK24026267\nSupervisor:\nDr. Zina Ibrahim (Zina.ibrahim@kcl.ac.uk)\n\n\n\n\n\n\n\n\nA short description of the project\n\n\n\n\n\nElectronic Health Records (EHRs) contain valuable clinical data (Key drawback: high missing rates). Masking strategy, such as random masking and block masking, is a key technique to simulate missing data. Its core principle is to generate training samples through the simulation missing mechanism, so that the model can better learn the data distribution (Qian et al. 2024).\nHowever, masking strategies are generally regarded as a fixed process when dealing with missing data, and it is unclear whether the consistency of different stages significantly affects model performance. In addition, research on imputation model optimization structures to improve prediction accuracy remains limited. This research aims to fill this research gap, explore the relationship between masking consistency and model performance, and provide optimization recommendations for future EHR data processing.\nIn this research, based on the “PyPOTS” open-source library created by Du (Du 2023), a variety of machine learning models (for instance, SAITS (Self-Attention-based Imputation for Time Series),BRITS (Bidirectional Recurrent Imputation for Time Series), Transformer etc.) are constructed to compare and evaluate the influence of mask strategy consistency on imputation quality and prediction performance (evaluation indicators such as accuracy, mean square error).\n\n\n\n\n\n\n\n\n\nAbstract\n\n\n\n\n\nBackground\nWith the widespread application of electronic health records (EHR) in clinical research and decision support, the high rate of missing data has become a significant challenge restricting the performance of data analysis and modelling of the complex time-series made available by this rich data resource. As a result, many deep learning models have been specifically formulated to impute complex multivariate time series data derived from EHRs.\nMasking is a pervasively used methodology for simulating missing data and is highly instrumental for the evaluation of deep learning models designed to impute data in multi- variate time-series. The current state of the art does not provide sufficient and systematic understand of how to utilise masking strategies during model training and inference. This study aims to explore the impact of the consistency of masking strategies in the training and inference stages of clinical EHR data on the filling of missing values and prediction performance. EHR data missing often has non-random, clinically-related structural characteristics, which poses challenges to data preprocessing and model robustness.\nMethods\nBased on the PhysioNet 2012 ICU patient dataset, using the PyPOTS framework, four types of masking strategies corresponding to different patterns of missingness in EHR time-series (random, temporal, spatial, and block) were evaluated in six deep learning models under different missing rates (20%, 30%, 50%). Experiments are conducted in both time series imputation and downstream classification tasks, using mean absolute error (MAE) and accuracy / F1 score as evaluation metrics, and the statistical significance of performance differences is analysed through paired t-tests.\nResults\nIn the imputation task, the block masking (where continuous blocks of missing data are observed in the time-series) exhibits the most significant performance variation under high missing rate conditions. Among the state of the art deep imputation models, BRITS and M-RNN are the most sensitive, while SAITS and CSAI demonstrate stronger robustness when the mask is inconsistent. In the classification task, the inconsistency of the mask has a relatively minor overall impact, only slightly improving the accuracy of SAITS and BRITS at low to medium missing rates, and maintaining stability for CSAI in all scenarios.\nConclusions\nMasking strategy consistency is not a universal requirement. Its selection should be based on the specific clinical context and the pattern of missingness observed in the dataset in hand. Structured masking corresponding to complex data-specific missingness patterns, combined with robust models, can better simulate the real missing mechanism and improve the reliability of clinical predictions. This study provides empirical evidence and methodological guidance for EHR data preprocessing and deep learning modelling in health informatics, which has extremely high clinical value.\n\n\n\n\n\n\n\n\n\nRelevant literature\n\n\n\nSome relevant literature and details can be found in the reference section.\n(Du 2023), (Qian, Ibrahim, and Dobson 2024), (Qian et al. 2024) etc..\n\n\nI had multiple meetings with my advisor, Dr. Zina, and also communicated with her via email. I also promptly reported the progress of the project to her at all times. Under the guidance of Zina, Linglong, and Joseph in the experimental operations, I successfully and excellently completed the experiments, the dissertation, and the presentation part of the defense. I hope to achieve satisfactory results for my master’s degree. This year’s research and professional courses have further deepened my insights in the field of data analysis and machine learning, as well as my ability to code and analyze rigorously. They have also strengthened my inspiration and motivation in academic research.\n\n\n\n\n\n\nResearch project poster\n\n\n\n\n\n\n\n\n\n\n\nSeminars Across the Four Domains\n\n\nPersonal Effectiveness:\nDuring the learning process of the seminar, I deeply realized the importance of personal effectiveness to the graduate stage. Time management and self-drive ability directly affect the quality of learning and research. Faced with the challenge of multiple modules and parallel research projects, I needed to plan my time more effectively and improve my productivity. In addition, critical thinking and self-reflection skills are the keys to improving personal effectiveness, which can help me evaluate my learning progress more comprehensively and constantly improve my research methods.\n\n\nEmployability:\nThe importance of vocational skills in academia and industry was highlighted through a series of seminars. As a graduate student majoring in applied statistical modeling and health informatics, I need to not only master solid statistical analysis and data modeling skills, but also improve communication and teamwork skills to adapt to interdisciplinary research and practical application needs. In addition, the discussion on industry trends in the seminar made me realize that I should actively accumulate practical experience, such as participating in or contacting health data analysis projects or internships as much as possible, so as to enhance my experience and competitiveness.\n\n\nAcademic Development:\nIn terms of academic development, I recognize that mastering effective literature reading and critical analysis skills is essential for research. How to choose high quality academic resources (Google Academics, KCL Library, etc.), improve writing skills, and conduct effective academic communication. At the same time, I am also aware that academic integrity and research ethics (avoidance of plagiarism) are particularly important in the field of health informatics and need to be strictly observed in the research process. In addition, the ability to collaborate across disciplines is also key to my academic development, helping me to more fully understand the challenges and opportunities of health data analysis.\n\n\nResearch Development:\nSeminars helped me to have a clearer understanding of the development direction of research ability. In the field of health informatics, data quality, privacy protection, and interpretability of models are all core issues in research. I need to continuously improve my programming skills, data processing ability and statistical modeling skills through different modules to ensure the reliability of research project results and other dimensions.\n\n\n\n\n\n\n\nSeminars Conclusion\n\n\n\nThese seminars not only deepened my understanding of my own academic and professional development, but also encouraged me to take the initiative to improve my personal ability and make continuous progress, which benefited me a lot in both my future research and employment environment.\n\n\n\n\n\n\nDu, Wenjie. 2023. “PyPOTS: A Python Toolbox for Data Mining on Partially-Observed Time Series.” arXiv Preprint arXiv:2305.18811.\n\n\nQian, Linglong, Zina Ibrahim, and Richard Dobson. 2024. “Uncertainty-Aware Deep Attention Recurrent Neural Network for Heterogeneous Time Series Imputation.” arXiv Preprint arXiv:2401.02258.\n\n\nQian, Linglong, Zina Ibrahim, Wenjie Du, Yiyuan Yang, and Richard JB Dobson. 2024. “Unveiling the Secrets: How Masking Strategies Shape Time Series Imputation.” arXiv Preprint arXiv:2405.17508.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reflective_model</span>"
    ]
  },
  {
    "objectID": "edu_career_goals.html",
    "href": "edu_career_goals.html",
    "title": "5  Education and Career Goals",
    "section": "",
    "text": "Smart Goal Setting\n\nObjective 1\n\n\n\n\n\n\nEnhance Programming and Technical Proficiency\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecific\n\nImprove programming skills (R, stata, python)\n\n\nMeasurable\n\nComplete all relevant coursework and assessments; undertake at least two online tutorials per week in R and Python; apply these skills in assignments and projects.\n\n\nAchievable\n\nPractice more coding exercise when have time. Apply these skills to weekly coursework and aim to complete additional coding exercises per language each month.\n\n\nRelevant\n\nReflect on progress fortnightly in the action plan section of the ePortfolio, linking directly to skills needed for the MSc project. Through these which will benefit for MSc project and professional skills.\n\n\nTime-bound\n\nAchieve intermediate milestones with each coursework submission; demonstrate proficiency by the end of each semesters.\n\n\n\n\n\n\nObjective 2\n\n\n\n\n\n\nImprove Communication and Language Skills\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecific\n\nExtracurricular practice and communication by using English. Enhance English communication skills for academic and social contexts.\n\n\nMeasurable\n\nParticipate in class discussions at least twice per session; engage in regular conversation groups; read and summarize one academic paper in English weekly.\n\n\nAchievable\n\nUtilize university resources such as language workshops; practice with peers and in everyday situations.\n\n\nRelevant\n\nImproved communication is crucial for collaboration, presentations, and future employability.\n\n\nTime-bound\n\nLong-term goals.\n\n\n\n\n\n\n\n\nCurrent Future Expectations and Plans\n\n\n\n\n\n\n\nFuture expectations and goals\n\n\n\nAfter the completion of my master’s degree, I plan to continue to study for a PhD degree, and I will also look for some suitable doctoral programs and research assistant, hoping to continue my in-depth study and research, so as to further develop my academic and professional skills and expand my horizons.\nIf there is no opportunity, I will return to my country to find a suitable job related to data analysis and processing, starting my career.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Education and Career Goals</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "6  Summary",
    "section": "",
    "text": "Summary\n\nIn conclusion, I am a sincere and warm-hearted scholar. I have a certain depth of understanding and skills in the fields of big data and data science, and I am actively and enthusiastically eager to continue conducting in-depth research in this field and make my own contribution to the world.\nThis eportfolio introduces Zhuoyuan Tang through introduction, CV, SWOT analysis, action plan, Reflective model (STARR), future goals and other aspects. These basically describe zhuoyuan’s experience, academic development, personality, hobbies, future goals, etc. Please refer to the specific corresponding sections. Nevertheless, the eportfolio will continue to be updated in terms of content and functionality. (To be continued……)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Du, W. (2023). PyPOTS: A python toolbox for data mining on\npartially-observed time series. arXiv Preprint\narXiv:2305.18811.\n\n\nQian, L., Ibrahim, Z., & Dobson, R. (2024). Uncertainty-aware deep\nattention recurrent neural network for heterogeneous time series\nimputation. arXiv Preprint arXiv:2401.02258.\n\n\nQian, L., Ibrahim, Z., Du, W., Yang, Y., & Dobson, R. J. (2024).\nUnveiling the secrets: How masking strategies shape time series\nimputation. arXiv Preprint arXiv:2405.17508.",
    "crumbs": [
      "References"
    ]
  }
]